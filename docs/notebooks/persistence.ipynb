{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Persistence Recipes\n",
    "\n",
    "In production, it is often necessary to persist data to disk. This notebook demonstrates how Bandit subclasses can be persisted to disk, reloaded, and even redefined on the fly.\n",
    "\n",
    "First, let's create a simple subclass of `Bandit` that will be trained a little, then persisted to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bayesianbandits import Arm, GammaRegressor, Agent, EpsilonGreedy\n",
    "\n",
    "arms = [\n",
    "    Arm(\"Action 1\", learner=GammaRegressor(alpha=1, beta=1)),\n",
    "    Arm(\"Action 2\", learner=GammaRegressor(alpha=1, beta=1)),\n",
    "]\n",
    "\n",
    "agent = Agent(arms, EpsilonGreedy(epsilon=0.1), random_seed=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll pull the arm once, update, and then persist the bandit to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned alpha and beta for arm 1: [2. 2.]\n"
     ]
    }
   ],
   "source": [
    "agent.pull()\n",
    "agent.update(np.atleast_1d(1))\n",
    "\n",
    "print(f\"Learned alpha and beta for arm 1: {agent.arms[0].learner.coef_[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joblib` is a great library for persisting objects to disk. It is a dependency of `scikit-learn`, so it is already installed when installing `bayesianbandits`.\n",
    "\n",
    "As we can see, the learned state of the bandit is persisted to disk. We can reload the bandit from disk, and it will be in the same state as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned alpha and beta for arm 1: [2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(agent, \"agent.pkl\")\n",
    "\n",
    "loaded: Agent[GammaRegressor, str, EpsilonGreedy] = joblib.load(\"agent.pkl\")\n",
    "\n",
    "print(f\"Learned alpha and beta for arm 1: {loaded.arms[0].learner.coef_[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After being reloaded, the bandit can be used as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned alpha and beta for arm 1: [2. 2.]\n",
      "Learned alpha and beta for arm 2: [2. 2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['agent.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded.pull()\n",
    "loaded.update(np.atleast_1d(1))\n",
    "\n",
    "print(f\"Learned alpha and beta for arm 1: {loaded.arms[0].learner.coef_[1]}\")\n",
    "print(f\"Learned alpha and beta for arm 2: {loaded.arms[1].learner.coef_[1]}\")\n",
    "\n",
    "joblib.dump(loaded, \"agent.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your learning session has gone on for some time, you may get an idea for a new arm. You want to try it out, but you don't want to lose the state of the bandit you've already learned. Fortunately, you can just redefine the `Bandit` subclass definition and reload the bandit from disk. Any arms in the new definition will be initialized when the bandit is reloaded.\n",
    "\n",
    "Note that the learned state of arm 1 is preserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned alpha and beta for arm 1: [2. 2.]\n",
      "Learned alpha and beta for arm 2: [2. 2.]\n",
      "Arms: [Arm(action_token=Action 1, reward_function=<function identity at 0x7f1505c33520>, Arm(action_token=Action 2, reward_function=<function identity at 0x7f1505c33520>, Arm(action_token=Action 3, reward_function=<function identity at 0x7f1505c33520>]\n"
     ]
    }
   ],
   "source": [
    "arm_3 = Arm(\"Action 3\", learner=GammaRegressor(alpha=1, beta=1))\n",
    "loaded_with_new_def: Agent[GammaRegressor, str, EpsilonGreedy] = joblib.load(\"agent.pkl\")\n",
    "loaded_with_new_def.add_arm(arm_3)\n",
    "\n",
    "print(f\"Learned alpha and beta for arm 1: {loaded_with_new_def.arms[0].learner.coef_[1]}\")\n",
    "print(f\"Learned alpha and beta for arm 2: {loaded_with_new_def.arms[1].learner.coef_[1]}\")\n",
    "\n",
    "print(f\"Arms: {loaded_with_new_def.arms}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the bandit can be used as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned alpha and beta for arm 1: [2. 2.]\n",
      "Learned alpha and beta for arm 2: [2. 2.]\n",
      "Learned alpha and beta for arm 3: [2. 2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['agent.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_with_new_def.pull()\n",
    "loaded_with_new_def.update(np.atleast_1d(1))\n",
    "\n",
    "print(f\"Learned alpha and beta for arm 1: {loaded_with_new_def.arms[0].learner.coef_[1]}\")\n",
    "print(f\"Learned alpha and beta for arm 2: {loaded_with_new_def.arms[1].learner.coef_[1]}\")\n",
    "print(f\"Learned alpha and beta for arm 3: {loaded_with_new_def.arms[2].learner.coef_[1]}\")\n",
    "\n",
    "joblib.dump(loaded_with_new_def, \"agent.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you may decide that `arm2` is not a good arm, and you want to remove it from the bandit. You can do this by redefining the `Bandit` subclass definition and reloading the bandit from disk. Any arms in the `Bandit` instance that are not in the new definition will be removed when the bandit is reloaded. \n",
    "\n",
    "Note that this is a destructive operation upon re-serialization, and the learned state of arm 1 is lost forever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arms: [Arm(action_token=Action 1, reward_function=<function identity at 0x7f1505c33520>, Arm(action_token=Action 2, reward_function=<function identity at 0x7f1505c33520>, Arm(action_token=Action 3, reward_function=<function identity at 0x7f1505c33520>]\n",
      "Learned alpha and beta for arm 1: [2. 2.]\n",
      "Learned alpha and beta for arm 3: [2. 2.]\n"
     ]
    }
   ],
   "source": [
    "loaded_with_removed_arm: Agent[GammaRegressor, str, EpsilonGreedy] = joblib.load(\"agent.pkl\")\n",
    "loaded_with_removed_arm.remove_arm(\"Action 2\")\n",
    "\n",
    "print(f\"Arms: {loaded_with_new_def.arms}\")\n",
    "\n",
    "print(f\"Learned alpha and beta for arm 1: {loaded_with_removed_arm.arms[0].learner.coef_[1]}\")\n",
    "print(f\"Learned alpha and beta for arm 3: {loaded_with_removed_arm.arms[1].learner.coef_[1]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
